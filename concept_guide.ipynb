{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8181e1",
   "metadata": {},
   "source": [
    "# üìÇ Upload-Driven RAG\n",
    "\n",
    "This notebook shows **every step**‚Äîfrom config to final answer download‚Äîof building an **Upload-Driven RAG** app in Streamlit where users can upload either their CSV or PDF files and LLM or hybrid ruling can answer it based on their uploaded files. Please dont run the code literally as this notebook only intended for explanation purposes. The steps: \n",
    "\n",
    "1. Making the data ingestion or insertion part while previewing it \n",
    "2. Converting both type of files into JSON \n",
    "3. Chunking the info and setting token limits to avoid sudden stop from LLM for ingesting too much info \n",
    "4. Constructing FAISS index to calculate the similarity distance \n",
    "5. Preparing the LLM model and ensuring the hybrid rules to ensure proper answer (pands or tiny snippet for number-based question and LLM for semantic questions) \n",
    "6. Some of the factors that might alter the answer through controlling the size of the info and how exact we want the those info to be\n",
    "\n",
    "## Step 1: Data Ingestion & Preview  \n",
    "**Concept**: Read a CSV or PDF, sample up to `MAX_ROWS`, and show a preview so users know they uploaded correctly and we don‚Äôt freeze on huge files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebffe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd, chardet\n",
    "from io import BytesIO\n",
    "import tabula \n",
    "\n",
    "# streamlit sidebar determining max rows from uploaded CSV will be used\n",
    "st.sidebar.header(\"‚öôÔ∏è Settings\")\n",
    "MAX_ROWS = st.sidebar.slider(\"üî¢ Max CSV rows to ingest\", 100, 20000, 1000, step=100)\n",
    "\n",
    "uploaded = st.file_uploader(\"Upload CSV or PDF\", type=[\"csv\",\"pdf\"])\n",
    "if not uploaded:\n",
    "    st.stop()\n",
    "\n",
    "# we can preview the top 5 of the CSV from the streamlit interface\n",
    "if uploaded.type == \"text/csv\":\n",
    "    raw = uploaded.read()\n",
    "    enc = chardet.detect(raw)[\"encoding\"]\n",
    "    df  = pd.read_csv(BytesIO(raw), encoding=enc, nrows=MAX_ROWS)\n",
    "    st.subheader(\"üìä CSV Preview\")\n",
    "    st.dataframe(df.head(5))\n",
    "\n",
    "# we can also preview the first page of the PDF from the streamlit interface\n",
    "else:\n",
    "    st.subheader(\"üìÑ PDF Preview (first page snippet)\")\n",
    "    raw_pdf = uploaded.read()\n",
    "    reader = PdfReader(BytesIO(raw_pdf))\n",
    "    pages = [p.extract_text() or \"\" for p in reader.pages]\n",
    "    st.text(pages[0][:500] + \"‚Ä¶\")\n",
    "\n",
    "    # OPTIONAL: we need to use tabulas if there are tables inside the PDF\n",
    "    try:\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(raw_pdf)\n",
    "        dfs = tabula.read_pdf(\"temp.pdf\", pages=\"all\", multiple_tables=True)\n",
    "        st.write(f\"Detected {len(dfs)} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0367358",
   "metadata": {},
   "source": [
    "## Step 2: JSON Conversion for Table-QA \n",
    "**Concept**: Convert the entire table (CSV or PDF-extracted) into JSON so numeric/date queries can be handled exactly (via Pandas) or via a small JSON snippet to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "texts, metas = [], []\n",
    "table_json = None \n",
    "\n",
    "# for CSV:\n",
    "table_json = json.dumps(df.to_dict(\"records\"), indent=2)\n",
    "with st.expander(\"üîé View CSV as JSON\"):\n",
    "    st.code(table_json, language=\"json\")\n",
    "\n",
    "# for PDF with tables:\n",
    "if dfs:\n",
    "    full_tables = {}\n",
    "    for ti, tdf in enumerate(dfs):\n",
    "        # show head\n",
    "        st.subheader(f\"üìã Table {ti+1} Preview\")\n",
    "        st.dataframe(tdf.head(5))\n",
    "        # JSON\n",
    "        j = json.dumps(tdf.to_dict(orient=\"records\"), indent=2)\n",
    "        full_tables[f\"table_{ti}\"] = tdf.to_dict(orient=\"records\")\n",
    "        with st.expander(f\"üîé View Table {ti+1} as JSON\"):\n",
    "            st.code(j, language=\"json\")\n",
    "        # flatten rows\n",
    "        for i, row in tdf.iterrows():\n",
    "            txt = \"; \".join(f\"{c}: {row[c]}\" for c in tdf.columns)\n",
    "            texts.append(txt)\n",
    "            metas.append((\"pdf_table\", i))\n",
    "        table_json = json.dumps(full_tables, indent=2)\n",
    "\n",
    "# for PDF without tables: \n",
    "for idx, pg in enumerate(pages):\n",
    "        if idx == 0:\n",
    "            st.text(pg[:500] + \"‚Ä¶\")\n",
    "        texts.append(pg)\n",
    "        metas.append((\"pdf_page\", idx))\n",
    "if table_json is None:\n",
    "        pages_list = [{\"page\": i+1, \"text\": pg[:2000]} for i, pg in enumerate(pages)]\n",
    "        table_json = json.dumps({\"pages\": pages_list}, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7942b1",
   "metadata": {},
   "source": [
    "## Step 3: Chunking & Token-Limit Enforcement\n",
    "**Concept**: Split each row or page of text into ‚â§512-token chunks so our embedding/RAG steps never overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e804101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# model init (cached)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", use_fast=True)\n",
    "embedder  = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "reranker  = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "def chunk_text(txt, chunk_size=200):\n",
    "    toks = tokenizer.tokenize(txt)\n",
    "    max_len = tokenizer.model_max_length - 2\n",
    "    step = min(chunk_size, max_len)\n",
    "    for i in range(0, len(toks), step):\n",
    "        yield tokenizer.convert_tokens_to_string(toks[i:i+step])\n",
    "\n",
    "# flatten all rows/pages into chunks\n",
    "texts, metas = [], []   # assume these were built in Step 1/2\n",
    "chunks, chunk_meta = [], []\n",
    "for (kind, idx), doc in zip(metas, texts):\n",
    "    for c in chunk_text(doc, chunk_size=200):\n",
    "        chunks.append(c)\n",
    "        chunk_meta.append((kind, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c6841",
   "metadata": {},
   "source": [
    "## Step 4: FAISS Index Construction\n",
    "**Concept**: Embed every chunk, then choose FlatL2 for small sets or IVF for larger, so ANN search is both fast and robust. Basically the **IndexFlatL2** counts the nearest-neighbors distance to all vectors while **IndexIVFFlat** is used to count the similarity distance between per centroid if we have enough data points (in this case, 39 points are the threshold per centroid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c00601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, numpy as np\n",
    "\n",
    "embs = embedder.encode(chunks, convert_to_numpy=True).astype(\"float32\")\n",
    "dim, n = embs.shape[1], embs.shape[0]\n",
    "nlist = min(100, n)\n",
    "\n",
    "# If too few vectors, use exact FLAT index\n",
    "if n < 39 * nlist:\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "else:\n",
    "    quant = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexIVFFlat(quant, dim, nlist, faiss.METRIC_L2)\n",
    "    index.train(embs); index.nprobe = min(10, nlist)\n",
    "\n",
    "index.add(embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f186e02",
   "metadata": {},
   "source": [
    "## Step 5: Hybrid Retrieval\n",
    "**Concept**: after we instantiate the free llm model with the openrouter (we used free mistral small 24B through openrouter), we will set the rules based on the given query. \n",
    "### Numeric/date queries \n",
    "- **CSV path**: finds all the number-columns in the table, picks the last one, calculates its maximum value, and prints it. \n",
    "- **Fallback path**: takes the first 20 rows of your table (in JSON) & asks the AI ‚ÄúHere‚Äôs these 20 rows‚Äîwhat‚Äôs your answer?‚Äù\n",
    "\n",
    "### Free-text queries \n",
    "1. **Search**: We convert your question into a format the computer can compare against our indexed text, then pull out 20 candidates.\n",
    "\n",
    "2. **Fallback guess**: If none of those 20 are ‚Äúgood enough‚Äù (distance < threshold), we let the AI guess an answer and search again to improve recall.\n",
    "\n",
    "3. **Rerank**: We ask a second, lightweight AI model to score how well each of those 20 snippets actually matches your question, then pick the top K of them.\n",
    "\n",
    "4. **Show context**: Those top K snippets are fed into the AI.\n",
    "\n",
    "5. **Answer**: LLM gives the answers based on the those top K snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\",\"\").strip()\n",
    "assert api_key, \"‚ùóÔ∏è Set OPENROUTER_API_KEY in .env or sidebar\"\n",
    "\n",
    "router = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = st.text_input(\"‚ùì Ask your question:\")\n",
    "if st.button(\"Retrieve & Answer\") and query:\n",
    "    lowq = query.lower()\n",
    "    # A) Numeric/Date\n",
    "    if any(k in lowq for k in (\"max\",\"min\",\"sum\",\"earliest\")):\n",
    "        # exact with Pandas\n",
    "        if uploaded.type==\"text/csv\":\n",
    "            numcols = df.select_dtypes(include=[np.number]).columns\n",
    "            if \"max\" in lowq and numcols.any():\n",
    "                col = numcols[-1]; mv = df[col].max()\n",
    "                st.write(f\"Max {col} = {mv}\"); st.stop()\n",
    "        # Otherwise, grab the first 20 rows as JSON and ask the AI directly\n",
    "        sample = json.loads(table_json)[:20]\n",
    "        prompt = f\"Here are 20 rows as JSON:\\n{json.dumps(sample,indent=2)}\\nQ: {query}\"\n",
    "        resp = router.chat.completions.create(\n",
    "            model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0\n",
    "        )\n",
    "        st.write(resp.choices[0].message.content); st.stop()\n",
    "\n",
    "    # B) Semantic RAG\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    D,I   = index.search(q_emb,20)\n",
    "    \n",
    "    # If nothing was similar enough, ask the AI to draft a quick answer (‚ÄúHyDE‚Äù)\n",
    "    if D.max() < 0.3:\n",
    "        pseudo = router.chat.completions.create(\n",
    "            model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "            messages=[{\"role\":\"user\",\"content\":f\"Answer briefly: {query}\"}],\n",
    "            temperature=0.7,max_tokens=100\n",
    "        ).choices[0].message.content\n",
    "        q_emb = embedder.encode([pseudo], convert_to_numpy=True).astype(\"float32\")\n",
    "        D,I   = index.search(q_emb,20)\n",
    "    \n",
    "    # Rerank those 20 by how well the AI thinks they match your question\n",
    "    cands  = [chunks[i] for i in I[0]]\n",
    "    scores = reranker.predict([[query,c] for c in cands])\n",
    "    \n",
    "    # Pick the top-K best snippets and show them in a collapsible box\n",
    "    k = min(TOP_K, len(cands))\n",
    "    top  = [cands[i] for i in np.argsort(scores)[-k:]]\n",
    "    with st.expander(\"üîç Retrieved Chunks\", expanded=False):\n",
    "        for t in top: st.markdown(f\"> {t}\")\n",
    "\n",
    "    # Send those top-K snippets + your question back to the AI for an actual answer\n",
    "    final_prompt = f\"Context:\\n{chr(10).join(top)}\\n\\nQ: {query}\"\n",
    "    gen = router.chat.completions.create(\n",
    "        model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        messages=[{\"role\":\"user\",\"content\":final_prompt}],\n",
    "        temperature=0.7, max_tokens=300\n",
    "    )\n",
    "    st.subheader(\"üí¨ Answer\"); st.write(gen.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c248d",
   "metadata": {},
   "source": [
    "# Optional infos \n",
    "\n",
    "We make additional sliders as info to allow users tune the **speed vs accuracy** with three key stages: chunk size, how many snippets, and how often we should guess/check \n",
    "\n",
    "## Chunk size (tokens)\n",
    "Controls how big each piece of text is when we break your document up:\n",
    "\n",
    "- Smaller means more pieces and more precise search, but more work to process.\n",
    "\n",
    "- Larger means fewer, longer pieces which resulted in faster process but can miss fine details.\n",
    "\n",
    "## Top-K texts\n",
    "How many of the best-matching pieces we show to the AI for the final answer.\n",
    "\n",
    "- Higher means more context, but may include irrelevant bits.\n",
    "\n",
    "- Lower means keeps the AI focused, but might leave out useful info.\n",
    "\n",
    "## HyDE threshold\n",
    "A ‚Äúpicky‚Äù number between 0 and 1 that says how good a match has to be before we skip the AI-guess step.\n",
    "\n",
    "- Lower threshold means we‚Äôre easy to please, so we rarely ask the AI to draft a pseudo-answer.\n",
    "\n",
    "- Higher threshold means we‚Äôre more demanding, so we fall back to a quick AI guess more often to widen our search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE  = st.sidebar.slider(\n",
    "    \"üìÑ Chunk size (tokens)\", 100, 500, 200,\n",
    "    help=\"Max number of tokens per text chunk. Smaller values mean finer-grained chunks (more precise retrieval) but a larger index.\"\n",
    ")\n",
    "st.sidebar.markdown(\"*(How many tokens each chunk can have. Smaller ‚Üí more chunks, finer retrieval.)*\")\n",
    "\n",
    "TOP_K       = st.sidebar.slider(\n",
    "    \"üîç top-K texts\", 1, 10, 5,\n",
    "    help=\"Number of top-retrieved chunks shown to the LLM. Higher values give more context but may include less relevant passages.\"\n",
    ")\n",
    "st.sidebar.markdown(\"*(How many retrieved snippets to pass to the model.)*\")\n",
    "\n",
    "HYDE_THRESH = st.sidebar.slider(\n",
    "    \"ü§ñ HyDE threshold\", 0.1, 0.9, 0.3, step=0.05,\n",
    "    help=\"Recall similarity cutoff below which a 'pseudo-answer' (HyDE) is generated to improve retrieval. Lower ‚Üí fewer HyDE calls.\"\n",
    ")\n",
    "st.sidebar.markdown(\"*(Cosine similarity below which we ask the LLM to draft a hypothetical answer.)*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbd22b",
   "metadata": {},
   "source": [
    "# More optional Infos: Temperatures vs HyDE Threshold \n",
    "## Temperature\n",
    "- When it applies: Only during the generation step, when you ask the LLM to produce text (HyDE pseudo-answers or the final answer).\n",
    "\n",
    "- What it does: Think of it like ‚Äúhow creative‚Äù you want the AI to be.\n",
    "\n",
    "- Low (0‚Äì0.3) ‚Üí very predictable, repeats the most likely phrasing.\n",
    "\n",
    "- Medium (0.4‚Äì0.7) ‚Üí a bit more variety, can reword or add small surprises.\n",
    "\n",
    "- High (0.8‚Äì1.0) ‚Üí lots of creativity, but also more risk of going off-topic or making stuff up.\n",
    "\n",
    "## HyDE Threshold\n",
    "- When it applies: In the retrieval stage, before generation.\n",
    "\n",
    "- What it does: Measures how ‚Äúclose‚Äù your question is to anything in the index.\n",
    "\n",
    "- After doing the first pass of similarity search, you look at the best match score. If that best match is below your HyDE threshold, it means ‚Äúwe‚Äôre not confident anything really matches,‚Äù so you ask the LLM to draft a pseudo-answer. You then re-search using that pseudo-answer to pull in more relevant chunks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
